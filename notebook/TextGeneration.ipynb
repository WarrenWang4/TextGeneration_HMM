{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "objective-liberty",
   "metadata": {},
   "source": [
    "# Text generation using Hidden Markov Model\n",
    "\n",
    "For this project I will generate new text and perform text prediction using the Hidden Markov Model based of ABC news headline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "structural-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "developed-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_df = pd.read_csv(\"../data/abcnews-date-text.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-finder",
   "metadata": {},
   "source": [
    "## Data cleaning \n",
    "\n",
    "As the data set is too large with over a million rows and it takes too long to run and train the model. I decided to sample 99999 rows out of the million+ rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adult-cache",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>390544</th>\n",
       "      <td>minister calls for upfront parking fine tactics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720474</th>\n",
       "      <td>sugar industry cant afford looming strike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97949</th>\n",
       "      <td>convent to play host to arts and cultural centre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595117</th>\n",
       "      <td>food shortages cruel kims birthday bash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9249</th>\n",
       "      <td>youths await sentencing over rape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018604</th>\n",
       "      <td>twitter australia md on the future and living ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83163</th>\n",
       "      <td>govt urged to consider light rail system for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875572</th>\n",
       "      <td>looming broiler code review promises more plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60366</th>\n",
       "      <td>recommendations made to improve safety for police</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122600</th>\n",
       "      <td>china lists of us goods it might hit with 25 p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99999 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline_text\n",
       "390544     minister calls for upfront parking fine tactics\n",
       "720474           sugar industry cant afford looming strike\n",
       "97949     convent to play host to arts and cultural centre\n",
       "595117             food shortages cruel kims birthday bash\n",
       "9249                     youths await sentencing over rape\n",
       "...                                                    ...\n",
       "1018604  twitter australia md on the future and living ...\n",
       "83163         govt urged to consider light rail system for\n",
       "875572   looming broiler code review promises more plan...\n",
       "60366    recommendations made to improve safety for police\n",
       "1122600  china lists of us goods it might hit with 25 p...\n",
       "\n",
       "[99999 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_df = headline_df[['headline_text']]\n",
    "headline_df = headline_df.sample(99999)\n",
    "headline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-portable",
   "metadata": {},
   "source": [
    "## Formulate ideas on how ML can be used to learn word correlations and distributions\n",
    "\n",
    "Idea 1: A possible Machine Learning algorithm that comes to mind that may be used to learn word correlations and distributions would be the K-means algorithm. By using K-means, we can distribute text and words that are correlated to each other into clusters. Therefore, basing the word prediction by determining which cluster a given word belongs to. (Just an idea)\n",
    "\n",
    "Idea 2: Another idea would be to use the Markov Model. As words and text are sequential data, representing its correlation and distribution using a Markov Model is intuitive. However, often times the states we want to understand are hidden such as part-of-speech tags when modeling text data. Therefore, by including hidden states (hence using the Hidden Markov Model) it allows us to use observed and hidden states as a factor when determining the probability of the next generated word. This is what we will be building for this project.\n",
    "\n",
    "## Building Hidden Markov Model\n",
    "\n",
    "### 1. Collecting all the different words from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "palestinian-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "headlines = headline_df['headline_text']\n",
    "\n",
    "for headline in headlines:\n",
    "    headline = headline.split()\n",
    "    for word in headline:\n",
    "        words.append(word)\n",
    "        \n",
    "distinct_words = list(set(words))\n",
    "distinct_words.append(None)  # Null State\n",
    "distinct_words_count = len(distinct_words)\n",
    "word_dict = {word: i for i, word in enumerate(distinct_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-multimedia",
   "metadata": {},
   "source": [
    "### 2. Initializing and defining the transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "composed-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_1, matrix_2 = np.zeros((distinct_words_count, distinct_words_count)), np.zeros((distinct_words_count, distinct_words_count))\n",
    "\n",
    "for headline in headlines:\n",
    "    data = headline.split()\n",
    "    for i in range(len(data)):\n",
    "        if i < len(data) - 1:\n",
    "            matrix_1[word_dict[data[i]]][word_dict[data[i + 1]]] += 1\n",
    "        else:\n",
    "            matrix_1[word_dict[data[i]]][distinct_words_count - 1] += 1\n",
    "\n",
    "        if i < len(data) - 2:\n",
    "            matrix_2[word_dict[data[i]]][word_dict[data[i + 2]]] += 1\n",
    "        else:\n",
    "            matrix_2[word_dict[data[i]]][distinct_words_count - 1] += 1\n",
    "\n",
    "matrix_1[distinct_words_count - 1][distinct_words_count - 1], matrix_2[distinct_words_count - 1][distinct_words_count - 1] = 1, 1\n",
    "\n",
    "for i in range(len(matrix_1)):\n",
    "    matrix_1[i], matrix_2[i]= matrix_1[i] / matrix_1[i].sum(), matrix_2[i] / matrix_2[i].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-judge",
   "metadata": {},
   "source": [
    "### 3. Implementing the hidden states\n",
    "\n",
    "Through the research papers I read, most authors uses part-of-speech tags as their hidden state when building a Hidden Markov Model for text generation. However, I am unsure how to implement part-of-speech tags as the hidden state as it is not being labeled in the dataset I chose.\n",
    "\n",
    "Therefore, I will approach it differently. For the purpose of this project I will have my hidden states as words that either start with a vowels or non-vowels. There are approximated 170,000 words that are currently being used in the english language and I will assume that 10% of them starts with a vowel. Therefore, the probability of a word being followed by another word that starts with a vowel is far lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dict = {'vowel': 0, 'non-vowel': 1}\n",
    "hidden_states = ['vowel','non-vowel']\n",
    "hidden_matrix = [[.8, .2],[.9, .1]]\n",
    "\n",
    "def emission(probability, state):\n",
    "    for i in range(len(probability)-1):\n",
    "        if state is 'vowel':\n",
    "            if distinct_words[i][0] == 'a' or distinct_words[i][0] == 'e' or distinct_words[i][0] == 'i' or distinct_words[i][0] == 'o' or distinct_words[i][0] == 'u':\n",
    "                probability[i] *= 2\n",
    "            else:\n",
    "                probability[i] /= 2\n",
    "        else:\n",
    "            if distinct_words[i][0] == 'a' or distinct_words[i][0] == 'e' or distinct_words[i][0] == 'i' or distinct_words[i][0] == 'o' or distinct_words[i][0] == 'u':\n",
    "                probability[i] /= 2\n",
    "            else:\n",
    "                probability[i] *= 2\n",
    "    probability[i] /= probability.sum()\n",
    "\n",
    "\n",
    "def nextHiddenState(hiddenState):\n",
    "    nextHiddenState = np.random.choice(hiddenStates, size=1, p=hiddenStateTransitionMatrix[hiddenStateDict[hiddenState]])\n",
    "    return nextHiddenState[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-impression",
   "metadata": {},
   "source": [
    "### 4. Sampling matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HMM(word_choice = None, length = 7, sentence = [], hidden_State = 'Short'):\n",
    "    if word_choice is None:\n",
    "        word_choice = np.random.choice(new_words, size=1)[0]\n",
    "    if sentence == []:\n",
    "        sentence.append(word_choice)   \n",
    "    next_word = np.random.choice(unique_words, size = 1,p = First_Trans_Mat[words[sentence[-1]]])[0]\n",
    "    if length > 1:\n",
    "        while(next_word is None):\n",
    "            next_word = np.random.choice(unique_words, size = 1,p = First_Trans_Mat[words[sentence[-1]]])[0]\n",
    "\n",
    "    while next_word is not None:\n",
    "        sentence.append(next_word)\n",
    "        next_Probs = First_Trans_Mat[words[sentence[-1]]] * (Second_Trans_Mat[words[sentence[-2]]]) + First_Trans_Mat[words[sentence[-1]]]/4\n",
    "        next_prob = emission(next_Probs, hidden_State)\n",
    "        next_Probs[-1] += 0.00001\n",
    "        next_Probs = next_Probs / next_Probs.sum()\n",
    "        \n",
    "        if len(sentence) < length - 1:\n",
    "            if next_Probs.sum() > next_Probs[-1]:\n",
    "                next_Probs[-1] = next_Probs[-1] / 10\n",
    "            next_Probs = next_Probs / next_Probs.sum()\n",
    "            \n",
    "        if len(sentence) > length + 1:\n",
    "            next_Probs[-1] = next_Probs[-1] * 2\n",
    "            next_Probs = next_Probs / next_Probs.sum()\n",
    "        next_word = np.random.choice(unique_words, size = 1,p = next_Probs)[0]\n",
    "        \n",
    "        hidden_State = nextState(hidden_State)\n",
    "        \n",
    "    return sentence\n",
    "\n",
    "def generateText(init_word, text_arr):\n",
    "    if init_word == \"\":\n",
    "        init_word = np.random.choice(distinct_word)\n",
    "    text_arr.append(init-word)\n",
    "    following_word = np.random.choice(distinct_words, p = matrix_1[word_dict[sentence[-1]]])\n",
    "    \n",
    "    for i in range(8):\n",
    "        sentence.append(next_word)\n",
    "        next_Probs = First_Trans_Mat[words[sentence[-1]]] * (Second_Trans_Mat[words[sentence[-2]]]) + First_Trans_Mat[words[sentence[-1]]]/4\n",
    "        next_prob = emission(next_Probs, hidden_State)\n",
    "        next_Probs[-1] += 0.00001\n",
    "        next_Probs = next_Probs / next_Probs.sum()\n",
    "        \n",
    "        if len(sentence) < length - 1:\n",
    "            if next_Probs.sum() > next_Probs[-1]:\n",
    "                next_Probs[-1] = next_Probs[-1] / 10\n",
    "            next_Probs = next_Probs / next_Probs.sum()\n",
    "            \n",
    "        if len(sentence) > length + 1:\n",
    "            next_Probs[-1] = next_Probs[-1] * 2\n",
    "            next_Probs = next_Probs / next_Probs.sum()\n",
    "        next_word = np.random.choice(unique_words, size = 1,p = next_Probs)[0]\n",
    "        \n",
    "        hidden_State = nextState(hidden_State)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
