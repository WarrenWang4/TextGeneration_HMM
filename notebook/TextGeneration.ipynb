{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "artistic-darkness",
   "metadata": {},
   "source": [
    "# Text generation using Hidden Markov Model\n",
    "\n",
    "For this project I will generate new text and perform text prediction using the Hidden Markov Model based of ABC news headline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surprised-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "champion-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_df = pd.read_csv(\"../data/abcnews-date-text.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-gazette",
   "metadata": {},
   "source": [
    "## Data cleaning \n",
    "\n",
    "As the data set is too large with over a million rows and it takes too long to run and train the model. I decided to sample 99999 rows out of the million+ rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "large-slope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217838</th>\n",
       "      <td>mp calls for desal plant funds to be diverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103594</th>\n",
       "      <td>williams davenport reach stanford semi finals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406293</th>\n",
       "      <td>call for regional cancer treatment centres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247914</th>\n",
       "      <td>fortescue to launch joint iron ore venure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795082</th>\n",
       "      <td>qld government seeks foreign interest in ecoto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460487</th>\n",
       "      <td>us fears over pakistan weapons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6575</th>\n",
       "      <td>us air assault division crosses into iraq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952709</th>\n",
       "      <td>poison chemical found on russian who died in u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157323</th>\n",
       "      <td>who recalls flu virus sent to labs worldwide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248170</th>\n",
       "      <td>us diplomacy efforts continue in north korea c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99999 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline_text\n",
       "217838      mp calls for desal plant funds to be diverted\n",
       "103594      williams davenport reach stanford semi finals\n",
       "406293         call for regional cancer treatment centres\n",
       "247914          fortescue to launch joint iron ore venure\n",
       "795082  qld government seeks foreign interest in ecoto...\n",
       "...                                                   ...\n",
       "460487                     us fears over pakistan weapons\n",
       "6575            us air assault division crosses into iraq\n",
       "952709  poison chemical found on russian who died in u...\n",
       "157323       who recalls flu virus sent to labs worldwide\n",
       "248170  us diplomacy efforts continue in north korea c...\n",
       "\n",
       "[99999 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_df = headline_df[['headline_text']]\n",
    "headline_df = headline_df.sample(99999)\n",
    "headline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-majority",
   "metadata": {},
   "source": [
    "## Formulate ideas on how ML can be used to learn word correlations and distributions\n",
    "\n",
    "Idea 1: A possible Machine Learning algorithm that comes to mind that may be used to learn word correlations and distributions would be the K-means algorithm. By using K-means, we can distribute text and words that are correlated to each other into clusters. Therefore, basing the word prediction by determining which cluster a given word belongs to. (Just an idea)\n",
    "\n",
    "Idea 2: Another idea would be to use the Markov Model. As words and text are sequential data, representing its correlation and distribution using a Markov Model is intuitive. However, often times the states we want to understand are hidden such as part-of-speech tags when modeling text data. Therefore, by including hidden states (hence using the Hidden Markov Model) it allows us to use observed and hidden states as a factor when determining the probability of the next generated word. This is what we will be building for this project.\n",
    "\n",
    "## Building Hidden Markov Model\n",
    "\n",
    "### 1. Collecting all the different words from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "higher-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "headlines = headline_df['headline_text']\n",
    "\n",
    "for headline in headlines:\n",
    "    headline = headline.split()\n",
    "    for word in headline:\n",
    "        words.append(word)\n",
    "        \n",
    "distinct_words = list(set(words))\n",
    "distinct_words.append(None)  # Null State\n",
    "distinct_words_count = len(distinct_words)\n",
    "word_dict = {word: i for i, word in enumerate(distinct_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-feelings",
   "metadata": {},
   "source": [
    "### 2. Initializing and defining the transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hazardous-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_1, matrix_2 = np.zeros((distinct_words_count, distinct_words_count)), np.zeros((distinct_words_count, distinct_words_count))\n",
    "\n",
    "for headline in headlines:\n",
    "    data = headline.split()\n",
    "    for i in range(len(data)):\n",
    "        if i < len(data) - 1:\n",
    "            matrix_1[word_dict[data[i]]][word_dict[data[i + 1]]] += 1\n",
    "        else:\n",
    "            matrix_1[word_dict[data[i]]][distinct_words_count - 1] += 1\n",
    "\n",
    "        if i < len(data) - 2:\n",
    "            matrix_2[word_dict[data[i]]][word_dict[data[i + 2]]] += 1\n",
    "        else:\n",
    "            matrix_2[word_dict[data[i]]][distinct_words_count - 1] += 1\n",
    "\n",
    "matrix_1[distinct_words_count - 1][distinct_words_count - 1], matrix_2[distinct_words_count - 1][distinct_words_count - 1] = 1, 1\n",
    "\n",
    "for i in range(len(matrix_1)):\n",
    "    matrix_1[i], matrix_2[i]= matrix_1[i] / matrix_1[i].sum(), matrix_2[i] / matrix_2[i].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-tourist",
   "metadata": {},
   "source": [
    "### 3. Implementing the hidden states\n",
    "\n",
    "Through the research papers I read, most authors uses part-of-speech tags as their hidden state when building a Hidden Markov Model for text generation. However, I am unsure how to implement part-of-speech tags as the hidden state as it is not being labeled in the dataset I chose.\n",
    "\n",
    "Therefore, I will approach it differently. For the purpose of this project I will have my hidden states as words that either start with a vowels or non-vowels. As there are only 5 vowels in the alphabet, I would assume the probability of a word being followed by another word that starts with a vowel is far lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fitting-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dict = {'vowel': 0, 'non-vowel': 1}\n",
    "hidden_states = ['vowel','non-vowel']\n",
    "hidden_matrix = [[.8, .2],[.9, .1]]\n",
    "\n",
    "def emission(probability, state):\n",
    "    for i in range(len(probability)-1):\n",
    "        if state == 'vowel':\n",
    "            if distinct_words[i][0] == 'a' or distinct_words[i][0] == 'e' or distinct_words[i][0] == 'i' or distinct_words[i][0] == 'o' or distinct_words[i][0] == 'u':\n",
    "                probability[i] *= 2\n",
    "            else:\n",
    "                probability[i] /= 2\n",
    "        else:\n",
    "            if distinct_words[i][0] == 'a' or distinct_words[i][0] == 'e' or distinct_words[i][0] == 'i' or distinct_words[i][0] == 'o' or distinct_words[i][0] == 'u':\n",
    "                probability[i] /= 2\n",
    "            else:\n",
    "                probability[i] *= 2\n",
    "    probability[i] /= probability.sum()\n",
    "    return probability\n",
    "\n",
    "\n",
    "def chooseHiddenState(hidden_state):\n",
    "    new_hidden_state = np.random.choice(hidden_states, size = 1, p = hidden_matrix[hidden_dict[hidden_state]])\n",
    "    return new_hidden_state[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-gospel",
   "metadata": {},
   "source": [
    "### 4. Sampling matrix to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "several-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateText(init_word, text_arr):\n",
    "    hidden_state = \"vowel\"\n",
    "    if init_word == \"\":\n",
    "        init_word = np.random.choice(distinct_words)\n",
    "        text_arr.append(init_word)\n",
    "    following_word = np.random.choice(distinct_words, p = matrix_1[word_dict[text_arr[-1]]])\n",
    "    \n",
    "    for i in range(8):\n",
    "        text_arr.append(following_word)\n",
    "        probability = matrix_1[word_dict[text_arr[-1]]]/4 + matrix_1[word_dict[text_arr[-1]]] * matrix_2[word_dict[text_arr[-2]]]\n",
    "        probability = emission(probability, hidden_state)\n",
    "        probability /= probability.sum()    \n",
    "        following_word = np.random.choice(distinct_words, p = probability)\n",
    "        hidden_state = chooseHiddenState(hidden_state)\n",
    "        \n",
    "    return text_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-crossing",
   "metadata": {},
   "source": [
    "### 5. Generating new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "impressed-material",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fires burn off for good samaritan killed in arizona \n"
     ]
    }
   ],
   "source": [
    "texts = generateText(\"\", [])\n",
    "output = \"\"\n",
    "for text in texts:\n",
    "    if text != None:\n",
    "        output = output + text + \" \"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-relationship",
   "metadata": {},
   "source": [
    "### 6. Predicting text given sequence of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "subject-specification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "israel arrest of al qaeda linked to close \n"
     ]
    }
   ],
   "source": [
    "texts = generateText(\"israel arrest\", [\"israel\", \"arrest\"])\n",
    "output = \"\"\n",
    "for text in texts:\n",
    "    if text != None:\n",
    "        output = output + text + \" \"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-roots",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As we can see, there are some relevancy towards the text being generated and predicted through the Hidden Markov Model. Overall, this has been a tough project, however, I am satisfied with the results of the Machine Learning algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
